"""LangGraph Research Agent.

A stateful agent that can autonomously search, fetch, and synthesize
information using the full tool set. Built on LangGraph StateGraph
with streaming support for FastAPI SSE endpoints.
"""

import json
import time
from typing import Any, AsyncGenerator, Dict, List, Literal, Optional

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, MessagesState, StateGraph
from loguru import logger

from app.core.config import settings
from app.services.ai.audit import AuditLogger
from app.services.ai.model_provider import get_chat_model
from app.services.ai.tools import create_tools_for_user


def _create_checkpointer():
    """Create the appropriate checkpointer based on config."""
    if settings.agent_checkpointer == "mongodb":
        try:
            from app.services.ai.checkpointer import MongoDBCheckpointer
            logger.info("Using MongoDB checkpointer for agent state persistence")
            return MongoDBCheckpointer()
        except Exception as e:
            logger.warning(f"MongoDB checkpointer init failed, falling back to memory: {e}")
    return MemorySaver()

RESEARCH_SYSTEM_PROMPT = """ä½ æ˜¯ News Hub çš„æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ã€‚

ä½ å¯ä»¥ä½¿ç”¨å¤šç§å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·è¿›è¡Œä¿¡æ¯ç ”ç©¶å’Œåˆ†æï¼š
- search_user_news: æœç´¢ç”¨æˆ·çš„æ–°é—»åº“
- get_recent_news: è·å–ç”¨æˆ·æœ€è¿‘çš„æ–°é—»
- web_search: æœç´¢äº’è”ç½‘è·å–æœ€æ–°ä¿¡æ¯ï¼ˆSearXNG/Tavilyï¼‰
- fetch_rss: ä¸»åŠ¨æŠ“å–RSS/Atomæºçš„æ–‡ç« åˆ—è¡¨
- scrape_webpage: æŠ“å–ç½‘é¡µæ­£æ–‡å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æ
- save_news_to_library: ä¿å­˜æ–°é—»åˆ°ç”¨æˆ·çš„æ–°é—»åº“
- list_sources / add_source / delete_source: ç®¡ç†è®¢é˜…æº
- list_tag_rules / add_tag_rule / delete_tag_rule: ç®¡ç†æ ‡ç­¾è§„åˆ™

å·¥ä½œæµç¨‹ï¼š
1. ç†è§£ç”¨æˆ·çš„ç ”ç©¶é—®é¢˜
2. åˆ¶å®šæœç´¢ç­–ç•¥ï¼ˆå…ˆæŸ¥æ–°é—»åº“ï¼Œä¸å¤Ÿå†æœäº’è”ç½‘ï¼‰
3. å¦‚éœ€æ·±å…¥äº†è§£ï¼ŒæŠ“å–å…³é”®ç½‘é¡µå†…å®¹
4. ç»¼åˆæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ç ”ç©¶å›ç­”
5. å¼•ç”¨æ¥æºæ—¶æä¾›æ ‡é¢˜å’Œé“¾æ¥

é‡è¦åŸåˆ™ï¼š
- ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·çš„æ–°é—»åº“
- åªåœ¨å¿…è¦æ—¶æœç´¢äº’è”ç½‘
- åŸºäºäº‹å®å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
- å¼•ç”¨æ¥æºæ—¶æä¾›æ ‡é¢˜å’Œé“¾æ¥
- æ‰§è¡Œåˆ é™¤ç­‰å±é™©æ“ä½œå‰ï¼Œå…ˆç¡®è®¤ç”¨æˆ·æ„å›¾
- å›ç­”è¦æœ‰æ¡ç†ï¼Œé€‚å½“ä½¿ç”¨åˆ—è¡¨å’Œåˆ†æ®µ
"""


class ResearchAgent:
    """LangGraph-based research agent with tool calling."""

    def __init__(self):
        self.audit = AuditLogger()
        self._checkpointer = _create_checkpointer()

    def _build_graph(self, user_id: str) -> Any:
        """Build a LangGraph StateGraph for the given user."""
        model = get_chat_model()
        if model is None:
            return None

        tools = create_tools_for_user(user_id)
        tools_by_name = {t.name: t for t in tools}
        model_with_tools = model.bind_tools(tools)

        async def reason(state: MessagesState) -> Dict[str, List]:
            """LLM reasoning node â€” decides whether to call tools or respond."""
            messages = [SystemMessage(content=RESEARCH_SYSTEM_PROMPT)] + state["messages"]
            response = await model_with_tools.ainvoke(messages)
            return {"messages": [response]}

        async def execute_tools(state: MessagesState) -> Dict[str, List]:
            """Execute all tool calls from the last AI message."""
            last_message = state["messages"][-1]
            results = []
            for tool_call in last_message.tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["args"]
                logger.info(f"Agent calling tool: {tool_name} args={tool_args}")
                try:
                    tool_fn = tools_by_name[tool_name]
                    observation = await tool_fn.ainvoke(tool_args)
                except Exception as e:
                    logger.error(f"Tool {tool_name} failed: {e}")
                    observation = json.dumps({"error": str(e)}, ensure_ascii=False)
                results.append(ToolMessage(content=observation, tool_call_id=tool_call["id"]))
            return {"messages": results}

        def should_continue(state: MessagesState) -> Literal["execute_tools", "__end__"]:
            """Route: if the LLM made tool calls, execute them; otherwise finish."""
            last = state["messages"][-1]
            if isinstance(last, AIMessage) and last.tool_calls:
                return "execute_tools"
            return END

        # Build the graph
        builder = StateGraph(MessagesState)
        builder.add_node("reason", reason)
        builder.add_node("execute_tools", execute_tools)
        builder.add_edge(START, "reason")
        builder.add_conditional_edges("reason", should_continue, ["execute_tools", END])
        builder.add_edge("execute_tools", "reason")

        return builder.compile(checkpointer=self._checkpointer)

    async def chat(
        self,
        messages: List[Dict[str, str]],
        user_id: str,
        thread_id: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """Stream agent responses.

        Yields text chunks as the agent reasons and calls tools.
        Compatible with FastAPI StreamingResponse.
        """
        t0 = time.monotonic()

        graph = self._build_graph(user_id)
        if graph is None:
            fallback = "AI åŠ©æ‰‹æš‚ä¸å¯ç”¨ï¼Œè¯·å…ˆé…ç½® OPENAI_API_KEYã€‚"
            yield fallback
            await self.audit.log(
                user_id=user_id,
                action="research_chat",
                input_summary=messages[-1].get("content", "")[:200] if messages else "",
                output_summary=fallback,
                latency_ms=int((time.monotonic() - t0) * 1000),
                fallback_used=True,
            )
            return

        # Convert dict messages to LangChain message objects
        lc_messages = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "user":
                lc_messages.append(HumanMessage(content=content))
            elif role == "assistant":
                lc_messages.append(AIMessage(content=content))

        config = {"configurable": {"thread_id": thread_id or user_id}}
        collected_text = []
        tool_calls_made = []

        try:
            async for event in graph.astream_events(
                {"messages": lc_messages}, config=config, version="v2"
            ):
                kind = event["event"]

                # Stream LLM tokens
                if kind == "on_chat_model_stream":
                    chunk = event["data"]["chunk"]
                    if hasattr(chunk, "content") and isinstance(chunk.content, str) and chunk.content:
                        collected_text.append(chunk.content)
                        yield chunk.content

                # Track tool calls for audit
                elif kind == "on_tool_start":
                    tool_name = event.get("name", "unknown")
                    tool_calls_made.append(tool_name)
                    yield f"\n[ğŸ” {tool_name}...]\n"

            await self.audit.log(
                user_id=user_id,
                action="research_chat",
                input_summary=messages[-1].get("content", "")[:200] if messages else "",
                output_summary="".join(collected_text)[:200],
                model=settings.agent_model,
                latency_ms=int((time.monotonic() - t0) * 1000),
            )

        except Exception as e:
            logger.error(f"Research agent failed: {e}")
            error_msg = f"æŠ±æ­‰ï¼Œå‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
            yield error_msg
            await self.audit.log(
                user_id=user_id,
                action="research_chat",
                input_summary=messages[-1].get("content", "")[:200] if messages else "",
                model=settings.agent_model,
                latency_ms=int((time.monotonic() - t0) * 1000),
                error=str(e),
            )

