"""
RAG-enabled AI Assistant with Elasticsearch

Implements Retrieval-Augmented Generation using:
- Elasticsearch for document retrieval
- Hybrid search (keyword + semantic)
- Function calling for dynamic retrieval
"""

import json
import time
from typing import Any, AsyncGenerator, Dict, List, Optional

from loguru import logger

from app.core.config import settings
from app.db.es import es_client
from app.db.mongo import mongodb
from app.services.ai.audit import AuditLogger
from app.services.ai.llm_client import get_llm_client
from app.services.search.search_service import SearchService


class RAGAssistant:
    """AI Assistant with RAG capabilities using Elasticsearch."""

    def __init__(self):
        self.client = get_llm_client()
        self.audit = AuditLogger()
        self.search_service = None
        if es_client.client:
            self.search_service = SearchService(es_client.client)

    def _get_tools_definition(self) -> List[Dict[str, Any]]:
        """Define tools that AI can call."""
        return [
            {
                "type": "function",
                "function": {
                    "name": "search_user_news",
                    "description": "æœç´¢ç”¨æˆ·çš„æ–°é—»åº“ã€‚å½“ç”¨æˆ·è¯¢é—®å…³äºä»–ä»¬çš„æ–°é—»ã€æ–‡ç« ã€è®¢é˜…å†…å®¹æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "æœç´¢å…³é”®è¯æˆ–é—®é¢˜",
                            },
                            "limit": {
                                "type": "integer",
                                "description": "è¿”å›ç»“æœæ•°é‡",
                                "default": 5,
                            },
                        },
                        "required": ["query"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "get_recent_news",
                    "description": "è·å–ç”¨æˆ·æœ€è¿‘çš„æ–°é—»ã€‚å½“ç”¨æˆ·è¯¢é—®'æœ€è¿‘æœ‰ä»€ä¹ˆæ–°é—»'ã€'ä»Šå¤©çš„æ–°é—»'ç­‰æ—¶ä½¿ç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "hours": {
                                "type": "integer",
                                "description": "è·å–æœ€è¿‘å¤šå°‘å°æ—¶çš„æ–°é—»",
                                "default": 24,
                            },
                            "limit": {
                                "type": "integer",
                                "description": "è¿”å›ç»“æœæ•°é‡",
                                "default": 10,
                            },
                        },
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "web_search",
                    "description": "æœç´¢äº’è”ç½‘è·å–æœ€æ–°ä¿¡æ¯ã€‚å½“ç”¨æˆ·è¯¢é—®çš„å†…å®¹ä¸åœ¨æ–°é—»åº“ä¸­ï¼Œæˆ–éœ€è¦æœ€æ–°ä¿¡æ¯æ—¶ä½¿ç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "æœç´¢å…³é”®è¯",
                            },
                            "max_results": {
                                "type": "integer",
                                "description": "è¿”å›ç»“æœæ•°é‡",
                                "default": 5,
                            },
                        },
                        "required": ["query"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "fetch_rss",
                    "description": "ä¸»åŠ¨æŠ“å–RSS/Atomæºçš„æ–‡ç« åˆ—è¡¨ã€‚å½“ç”¨æˆ·æä¾›RSSé“¾æ¥æˆ–è¦æ±‚æŠ“å–æŸä¸ªæºæ—¶ä½¿ç”¨ã€‚åªè¿”å›ç»“æœä¸ä¿å­˜ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "url": {
                                "type": "string",
                                "description": "RSS/Atomæºçš„URL",
                            },
                            "limit": {
                                "type": "integer",
                                "description": "è¿”å›æ–‡ç« æ•°é‡ä¸Šé™",
                                "default": 10,
                            },
                        },
                        "required": ["url"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "scrape_webpage",
                    "description": "æŠ“å–ç½‘é¡µæ­£æ–‡å†…å®¹ã€‚å½“ç”¨æˆ·æä¾›ç½‘é¡µé“¾æ¥å¹¶è¦æ±‚åˆ†æå†…å®¹æ—¶ä½¿ç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "url": {
                                "type": "string",
                                "description": "ç½‘é¡µURL",
                            },
                        },
                        "required": ["url"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "save_news_to_library",
                    "description": "ä¿å­˜ä¸€æ¡æ–°é—»åˆ°ç”¨æˆ·çš„æ–°é—»åº“ã€‚å½“ç”¨æˆ·è¦æ±‚ä¿å­˜ã€æ”¶è—æŸæ¡æ–°é—»æ—¶ä½¿ç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "title": {
                                "type": "string",
                                "description": "æ–°é—»æ ‡é¢˜",
                            },
                            "url": {
                                "type": "string",
                                "description": "æ–°é—»é“¾æ¥",
                            },
                            "description": {
                                "type": "string",
                                "description": "æ–°é—»æ‘˜è¦",
                                "default": "",
                            },
                            "source_name": {
                                "type": "string",
                                "description": "æ¥æºåç§°",
                                "default": "AIåŠ©æ‰‹",
                            },
                        },
                        "required": ["title", "url"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "list_sources",
                    "description": "åˆ—å‡ºç”¨æˆ·çš„æ‰€æœ‰è®¢é˜…æºã€‚å½“ç”¨æˆ·è¯¢é—®'æˆ‘çš„è®¢é˜…æº'ã€'æˆ‘è®¢é˜…äº†ä»€ä¹ˆ'æ—¶ä½¿ç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {},
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "add_source",
                    "description": "æ·»åŠ æ–°çš„è®¢é˜…æºã€‚å½“ç”¨æˆ·è¦æ±‚è®¢é˜…æ–°çš„RSSæºæˆ–æ–°é—»æºæ—¶ä½¿ç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "name": {
                                "type": "string",
                                "description": "è®¢é˜…æºåç§°",
                            },
                            "url": {
                                "type": "string",
                                "description": "è®¢é˜…æºURL",
                            },
                            "source_type": {
                                "type": "string",
                                "description": "æºç±»å‹ï¼šrss æˆ– api",
                                "default": "rss",
                            },
                        },
                        "required": ["name", "url"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "delete_source",
                    "description": "åˆ é™¤ä¸€ä¸ªè®¢é˜…æºåŠå…¶å…³è”æ–°é—»ã€‚å½“ç”¨æˆ·è¦æ±‚å–æ¶ˆè®¢é˜…æˆ–åˆ é™¤æŸä¸ªæºæ—¶ä½¿ç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "source_id": {
                                "type": "string",
                                "description": "è¦åˆ é™¤çš„è®¢é˜…æºID",
                            },
                        },
                        "required": ["source_id"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "list_tag_rules",
                    "description": "åˆ—å‡ºç”¨æˆ·çš„æ‰€æœ‰æ ‡ç­¾è§„åˆ™ã€‚å½“ç”¨æˆ·è¯¢é—®'æˆ‘çš„æ ‡ç­¾'ã€'æ ‡ç­¾è§„åˆ™'æ—¶ä½¿ç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {},
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "add_tag_rule",
                    "description": "åˆ›å»ºæ–°çš„è‡ªåŠ¨æ ‡ç­¾è§„åˆ™ã€‚å½“ç”¨æˆ·è¦æ±‚åˆ›å»ºæ ‡ç­¾ã€è®¾ç½®è‡ªåŠ¨åˆ†ç±»æ—¶ä½¿ç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "tag_name": {
                                "type": "string",
                                "description": "æ ‡ç­¾åç§°",
                            },
                            "keywords": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "åŒ¹é…å…³é”®è¯åˆ—è¡¨",
                            },
                        },
                        "required": ["tag_name", "keywords"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "delete_tag_rule",
                    "description": "åˆ é™¤ä¸€ä¸ªæ ‡ç­¾è§„åˆ™ã€‚å½“ç”¨æˆ·è¦æ±‚åˆ é™¤æŸä¸ªæ ‡ç­¾è§„åˆ™æ—¶ä½¿ç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "rule_id": {
                                "type": "string",
                                "description": "è¦åˆ é™¤çš„è§„åˆ™ID",
                            },
                        },
                        "required": ["rule_id"],
                    },
                },
            },
        ]

    async def _handle_search_user_news(
        self, user_id: str, query: str, limit: int = 5
    ) -> Dict[str, Any]:
        """Handle search_user_news tool call."""
        if not self.search_service:
            return {"error": "Elasticsearch not available", "results": []}

        try:
            response = await self.search_service.search(
                user_id=user_id,
                query=query,
                search_type="hybrid",
                page_size=limit,
            )

            results = [
                {
                    "title": r.title,
                    "url": r.url,
                    "description": r.description or "",
                    "source": r.source_name,
                    "published_at": (
                        r.published_at.isoformat() if r.published_at else None
                    ),
                    "score": round(r.score, 3),
                }
                for r in response.results
            ]

            return {
                "query": query,
                "total": response.total,
                "results": results,
                "took_ms": response.took_ms,
            }
        except Exception as e:
            logger.error(f"Search user news failed: {e}")
            return {"error": str(e), "results": []}

    async def _handle_get_recent_news(
        self, user_id: str, hours: int = 24, limit: int = 10
    ) -> Dict[str, Any]:
        """Handle get_recent_news tool call."""
        try:
            from datetime import datetime, timedelta

            start_date = datetime.utcnow() - timedelta(hours=hours)

            cursor = (
                mongodb.db.news.find(
                    {
                        "user_id": user_id,
                        "crawled_at": {"$gte": start_date},
                    }
                )
                .sort("crawled_at", -1)
                .limit(limit)
            )

            docs = await cursor.to_list(length=limit)

            results = [
                {
                    "title": doc.get("title", ""),
                    "url": doc.get("url", ""),
                    "description": doc.get("description", ""),
                    "source": doc.get("source_name", ""),
                    "crawled_at": doc.get("crawled_at").isoformat()
                    if doc.get("crawled_at")
                    else None,
                }
                for doc in docs
            ]

            return {
                "hours": hours,
                "count": len(results),
                "results": results,
            }
        except Exception as e:
            logger.error(f"Get recent news failed: {e}")
            return {"error": str(e), "results": []}

    async def _handle_web_search(
        self, query: str, max_results: int = 5
    ) -> Dict[str, Any]:
        """Handle web_search tool call."""
        try:
            from app.services.ai.web_search import WebSearchClient

            client = WebSearchClient()
            if not client.available:
                return {
                    "error": "External search not configured",
                    "results": [],
                }

            results = await client.search(query, max_results=max_results)

            return {
                "query": query,
                "count": len(results),
                "results": [
                    {
                        "title": r.get("title", ""),
                        "url": r.get("url", ""),
                        "description": r.get("description", ""),
                        "score": r.get("score", 0),
                    }
                    for r in results
                ],
            }
        except Exception as e:
            logger.error(f"Web search failed: {e}")
            return {"error": str(e), "results": []}

    async def _handle_fetch_rss(
        self, url: str, limit: int = 10
    ) -> Dict[str, Any]:
        """Handle fetch_rss tool call."""
        try:
            from app.services.collector.factory import CollectorFactory

            source_config = {
                "url": url,
                "source_type": "rss",
                "name": "AIä¸´æ—¶æŠ“å–",
                "user_id": "temp",
            }
            result = await CollectorFactory.collect(source_config)
            if not result.success:
                return {"error": result.error_message or "æŠ“å–å¤±è´¥", "items": []}

            items = [
                {
                    "title": item.title,
                    "url": item.url,
                    "description": item.description or "",
                    "published_at": (
                        item.published_at.isoformat() if item.published_at else None
                    ),
                    "author": item.author or "",
                }
                for item in result.items[:limit]
            ]
            return {"url": url, "count": len(items), "items": items}
        except Exception as e:
            logger.error(f"Fetch RSS failed: {e}")
            return {"error": str(e), "items": []}

    async def _handle_scrape_webpage(self, url: str) -> Dict[str, Any]:
        """Handle scrape_webpage tool call."""
        try:
            import httpx
            from bs4 import BeautifulSoup

            async with httpx.AsyncClient(
                timeout=15, follow_redirects=True
            ) as client:
                resp = await client.get(url, headers={
                    "User-Agent": "Mozilla/5.0 (compatible; NewsHub/1.0)"
                })
                resp.raise_for_status()

            soup = BeautifulSoup(resp.text, "html.parser")

            # Remove script/style elements
            for tag in soup(["script", "style", "nav", "footer", "header"]):
                tag.decompose()

            title = soup.title.string.strip() if soup.title and soup.title.string else ""

            # Try to find main content
            article = soup.find("article") or soup.find("main") or soup.body
            text = article.get_text(separator="\n", strip=True) if article else ""

            # Truncate to avoid token overflow
            if len(text) > 3000:
                text = text[:3000] + "...(å·²æˆªæ–­)"

            return {"url": url, "title": title, "content": text}
        except Exception as e:
            logger.error(f"Scrape webpage failed: {e}")
            return {"error": str(e), "title": "", "content": ""}

    async def _handle_save_news(
        self,
        user_id: str,
        title: str,
        url: str,
        description: str = "",
        source_name: str = "AIåŠ©æ‰‹",
    ) -> Dict[str, Any]:
        """Handle save_news_to_library tool call."""
        try:
            from datetime import datetime

            from bson import ObjectId

            from app.services.search.indexer import ESIndexer
            from app.services.tagging.rule_matcher import RuleMatcher
            from app.services.tagging.tag_service import TagService

            # Auto-tag
            tag_service = TagService(mongodb.db)
            rules = await tag_service.list_rules(user_id)
            matcher = RuleMatcher(rules)
            tags, matched_rule_ids = matcher.match(title, description)

            if matched_rule_ids:
                await tag_service.increment_match_count(matched_rule_ids)

            news_doc = {
                "_id": ObjectId(),
                "user_id": user_id,
                "title": title,
                "url": url,
                "description": description,
                "source_name": source_name,
                "tags": tags,
                "is_read": False,
                "is_starred": False,
                "crawled_at": datetime.utcnow(),
            }

            await mongodb.db.news.insert_one(news_doc)

            # Index to ES
            if es_client.client:
                indexer = ESIndexer(es_client.client)
                await indexer.index_news_item(
                    user_id=user_id,
                    news_id=str(news_doc["_id"]),
                    doc={
                        "title": title,
                        "url": url,
                        "description": description,
                        "source_name": source_name,
                        "tags": tags,
                    },
                )

            return {
                "success": True,
                "news_id": str(news_doc["_id"]),
                "tags": tags,
                "message": f"å·²ä¿å­˜: {title}",
            }
        except Exception as e:
            logger.error(f"Save news failed: {e}")
            return {"error": str(e), "success": False}

    async def _handle_list_sources(self, user_id: str) -> Dict[str, Any]:
        """Handle list_sources tool call."""
        try:
            cursor = mongodb.db.sources.find({"user_id": user_id}).sort(
                "created_at", -1
            )
            docs = await cursor.to_list(length=50)

            sources = [
                {
                    "id": str(doc["_id"]),
                    "name": doc.get("name", ""),
                    "url": doc.get("url", ""),
                    "source_type": doc.get("source_type", ""),
                    "status": doc.get("status", ""),
                    "article_count": doc.get("article_count", 0),
                }
                for doc in docs
            ]
            return {"count": len(sources), "sources": sources}
        except Exception as e:
            logger.error(f"List sources failed: {e}")
            return {"error": str(e), "sources": []}

    async def _handle_add_source(
        self, user_id: str, name: str, url: str, source_type: str = "rss"
    ) -> Dict[str, Any]:
        """Handle add_source tool call."""
        try:
            from datetime import datetime

            from bson import ObjectId

            source_doc = {
                "_id": ObjectId(),
                "user_id": user_id,
                "name": name,
                "url": url,
                "source_type": source_type,
                "status": "active",
                "article_count": 0,
                "error_count": 0,
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
            }
            await mongodb.db.sources.insert_one(source_doc)
            return {
                "success": True,
                "source_id": str(source_doc["_id"]),
                "message": f"å·²æ·»åŠ è®¢é˜…æº: {name}",
            }
        except Exception as e:
            logger.error(f"Add source failed: {e}")
            return {"error": str(e), "success": False}

    async def _handle_delete_source(
        self, user_id: str, source_id: str
    ) -> Dict[str, Any]:
        """Handle delete_source tool call."""
        try:
            from bson import ObjectId

            oid = ObjectId(source_id)
            result = await mongodb.db.sources.delete_one(
                {"_id": oid, "user_id": user_id}
            )
            if result.deleted_count == 0:
                return {"success": False, "message": "æœªæ‰¾åˆ°è¯¥è®¢é˜…æº"}

            # Delete associated news
            del_news = await mongodb.db.news.delete_many(
                {"source_id": source_id, "user_id": user_id}
            )

            # Delete from ES
            if es_client.client:
                from app.services.search.indexer import ESIndexer

                indexer = ESIndexer(es_client.client)
                await indexer.delete_by_source(user_id, source_id)

            return {
                "success": True,
                "message": f"å·²åˆ é™¤è®¢é˜…æºï¼ŒåŒæ—¶åˆ é™¤äº† {del_news.deleted_count} æ¡å…³è”æ–°é—»",
            }
        except Exception as e:
            logger.error(f"Delete source failed: {e}")
            return {"error": str(e), "success": False}

    async def _handle_list_tag_rules(self, user_id: str) -> Dict[str, Any]:
        """Handle list_tag_rules tool call."""
        try:
            from app.services.tagging.tag_service import TagService

            tag_service = TagService(mongodb.db)
            rules = await tag_service.list_rules(user_id)

            items = [
                {
                    "id": str(r["_id"]),
                    "tag_name": r.get("tag_name", ""),
                    "keywords": r.get("keywords", []),
                    "match_mode": r.get("match_mode", "any"),
                    "is_active": r.get("is_active", True),
                    "match_count": r.get("match_count", 0),
                }
                for r in rules
            ]
            return {"count": len(items), "rules": items}
        except Exception as e:
            logger.error(f"List tag rules failed: {e}")
            return {"error": str(e), "rules": []}

    async def _handle_add_tag_rule(
        self, user_id: str, tag_name: str, keywords: List[str]
    ) -> Dict[str, Any]:
        """Handle add_tag_rule tool call."""
        try:
            from app.schemas.tag import TagRuleCreate
            from app.services.tagging.tag_service import TagService

            tag_service = TagService(mongodb.db)
            data = TagRuleCreate(tag_name=tag_name, keywords=keywords)
            rule = await tag_service.create_rule(user_id, data)
            return {
                "success": True,
                "rule_id": str(rule["_id"]),
                "message": f"å·²åˆ›å»ºæ ‡ç­¾è§„åˆ™: {tag_name} (å…³é”®è¯: {', '.join(keywords)})",
            }
        except Exception as e:
            logger.error(f"Add tag rule failed: {e}")
            return {"error": str(e), "success": False}

    async def _handle_delete_tag_rule(
        self, user_id: str, rule_id: str
    ) -> Dict[str, Any]:
        """Handle delete_tag_rule tool call."""
        try:
            from app.services.tagging.tag_service import TagService

            tag_service = TagService(mongodb.db)
            deleted = await tag_service.delete_rule(rule_id, user_id)
            if not deleted:
                return {"success": False, "message": "æœªæ‰¾åˆ°è¯¥æ ‡ç­¾è§„åˆ™"}
            return {"success": True, "message": "å·²åˆ é™¤æ ‡ç­¾è§„åˆ™"}
        except Exception as e:
            logger.error(f"Delete tag rule failed: {e}")
            return {"error": str(e), "success": False}

    async def _execute_tool_call(
        self, tool_name: str, arguments: Dict[str, Any], user_id: str
    ) -> Dict[str, Any]:
        """Execute a tool call and return the result."""
        if tool_name == "search_user_news":
            return await self._handle_search_user_news(
                user_id=user_id,
                query=arguments.get("query", ""),
                limit=arguments.get("limit", 5),
            )
        elif tool_name == "get_recent_news":
            return await self._handle_get_recent_news(
                user_id=user_id,
                hours=arguments.get("hours", 24),
                limit=arguments.get("limit", 10),
            )
        elif tool_name == "web_search":
            return await self._handle_web_search(
                query=arguments.get("query", ""),
                max_results=arguments.get("max_results", 5),
            )
        elif tool_name == "fetch_rss":
            return await self._handle_fetch_rss(
                url=arguments.get("url", ""),
                limit=arguments.get("limit", 10),
            )
        elif tool_name == "scrape_webpage":
            return await self._handle_scrape_webpage(
                url=arguments.get("url", ""),
            )
        elif tool_name == "save_news_to_library":
            return await self._handle_save_news(
                user_id=user_id,
                title=arguments.get("title", ""),
                url=arguments.get("url", ""),
                description=arguments.get("description", ""),
                source_name=arguments.get("source_name", "AIåŠ©æ‰‹"),
            )
        elif tool_name == "list_sources":
            return await self._handle_list_sources(user_id=user_id)
        elif tool_name == "add_source":
            return await self._handle_add_source(
                user_id=user_id,
                name=arguments.get("name", ""),
                url=arguments.get("url", ""),
                source_type=arguments.get("source_type", "rss"),
            )
        elif tool_name == "delete_source":
            return await self._handle_delete_source(
                user_id=user_id,
                source_id=arguments.get("source_id", ""),
            )
        elif tool_name == "list_tag_rules":
            return await self._handle_list_tag_rules(user_id=user_id)
        elif tool_name == "add_tag_rule":
            return await self._handle_add_tag_rule(
                user_id=user_id,
                tag_name=arguments.get("tag_name", ""),
                keywords=arguments.get("keywords", []),
            )
        elif tool_name == "delete_tag_rule":
            return await self._handle_delete_tag_rule(
                user_id=user_id,
                rule_id=arguments.get("rule_id", ""),
            )
        else:
            return {"error": f"Unknown tool: {tool_name}"}

    async def chat_with_rag(
        self,
        messages: List[Dict[str, str]],
        user_id: str,
        max_iterations: int = 5,
    ) -> AsyncGenerator[str, None]:
        """
        Chat with RAG capabilities.

        The AI can autonomously decide when to retrieve information from:
        - User's news library (Elasticsearch)
        - Recent news (MongoDB)
        - External web search (Tavily)
        """
        t0 = time.monotonic()

        if self.client is None:
            fallback = "AI åŠ©æ‰‹æš‚ä¸å¯ç”¨ï¼Œè¯·å…ˆé…ç½® OPENAI_API_KEYã€‚"
            yield fallback
            await self.audit.log(
                user_id=user_id,
                action="rag_chat",
                input_summary=messages[-1].get("content", "")[:200] if messages else "",
                output_summary=fallback,
                latency_ms=int((time.monotonic() - t0) * 1000),
                fallback_used=True,
            )
            return

        # System prompt for RAG
        system_prompt = """ä½ æ˜¯ News Hub çš„æ™ºèƒ½æ–°é—»åŠ©æ‰‹ã€‚

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ï¼š
1. search_user_news - æœç´¢ç”¨æˆ·çš„æ–°é—»åº“
2. get_recent_news - è·å–ç”¨æˆ·æœ€è¿‘çš„æ–°é—»
3. web_search - æœç´¢äº’è”ç½‘è·å–æœ€æ–°ä¿¡æ¯
4. fetch_rss - ä¸»åŠ¨æŠ“å–RSS/Atomæºçš„æ–‡ç« åˆ—è¡¨
5. scrape_webpage - æŠ“å–ç½‘é¡µæ­£æ–‡å†…å®¹è¿›è¡Œåˆ†æ
6. save_news_to_library - ä¿å­˜æ–°é—»åˆ°ç”¨æˆ·çš„æ–°é—»åº“
7. list_sources - åˆ—å‡ºç”¨æˆ·çš„æ‰€æœ‰è®¢é˜…æº
8. add_source - æ·»åŠ æ–°çš„è®¢é˜…æº
9. delete_source - åˆ é™¤è®¢é˜…æºåŠå…¶å…³è”æ–°é—»
10. list_tag_rules - åˆ—å‡ºç”¨æˆ·çš„æ ‡ç­¾è§„åˆ™
11. add_tag_rule - åˆ›å»ºæ–°çš„è‡ªåŠ¨æ ‡ç­¾è§„åˆ™
12. delete_tag_rule - åˆ é™¤æ ‡ç­¾è§„åˆ™

å·¥ä½œæµç¨‹ï¼š
1. ç†è§£ç”¨æˆ·çš„é—®é¢˜
2. åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢ä¿¡æ¯æˆ–æ‰§è¡Œæ“ä½œ
3. é€‰æ‹©åˆé€‚çš„å·¥å…·æ‰§è¡Œ
4. åŸºäºç»“æœå›ç­”é—®é¢˜æˆ–ç¡®è®¤æ“ä½œ

é‡è¦åŸåˆ™ï¼š
- ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·çš„æ–°é—»åº“
- åªåœ¨å¿…è¦æ—¶æœç´¢äº’è”ç½‘
- åŸºäºäº‹å®å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
- å¼•ç”¨æ¥æºæ—¶æä¾›æ ‡é¢˜å’Œé“¾æ¥
- æ‰§è¡Œåˆ é™¤ç­‰å±é™©æ“ä½œå‰ï¼Œå…ˆç¡®è®¤ç”¨æˆ·æ„å›¾
"""

        conversation = [{"role": "system", "content": system_prompt}, *messages]
        tools = self._get_tools_definition()

        collected_chunks: List[str] = []
        tool_calls_made: List[str] = []

        try:
            for iteration in range(max_iterations):
                # Call LLM
                response = await self.client.chat.completions.create(
                    model=settings.openai_model,
                    messages=conversation,
                    tools=tools,
                    tool_choice="auto",
                    stream=False,  # We'll handle streaming separately
                )

                message = response.choices[0].message

                # If no tool calls, return the response
                if not message.tool_calls:
                    content = message.content or ""
                    collected_chunks.append(content)
                    yield content

                    # Log audit
                    await self.audit.log(
                        user_id=user_id,
                        action="rag_chat",
                        input_summary=messages[-1].get("content", "")[:200]
                        if messages
                        else "",
                        output_summary=content[:200],
                        model=settings.openai_model,
                        latency_ms=int((time.monotonic() - t0) * 1000),
                    )
                    return

                # AI wants to call tools
                conversation.append(
                    {
                        "role": "assistant",
                        "content": message.content,
                        "tool_calls": [
                            {
                                "id": tc.id,
                                "type": "function",
                                "function": {
                                    "name": tc.function.name,
                                    "arguments": tc.function.arguments,
                                },
                            }
                            for tc in message.tool_calls
                        ],
                    }
                )

                # Execute all tool calls
                for tool_call in message.tool_calls:
                    function_name = tool_call.function.name
                    function_args = json.loads(tool_call.function.arguments)

                    logger.info(
                        f"AI calling tool: {function_name} with args: {function_args}"
                    )
                    tool_calls_made.append(function_name)

                    # Execute tool
                    result = await self._execute_tool_call(
                        tool_name=function_name,
                        arguments=function_args,
                        user_id=user_id,
                    )

                    # Add tool result to conversation
                    conversation.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": json.dumps(result, ensure_ascii=False),
                        }
                    )

                    # Yield a status message to user
                    status = f"\n[ğŸ” æ­£åœ¨{function_name}...]\n"
                    yield status

            # Max iterations reached
            error_msg = "æŠ±æ­‰ï¼Œå¤„ç†è¶…æ—¶ã€‚è¯·å°è¯•ç®€åŒ–é—®é¢˜ã€‚"
            yield error_msg

            await self.audit.log(
                user_id=user_id,
                action="rag_chat",
                input_summary=messages[-1].get("content", "")[:200] if messages else "",
                output_summary=error_msg,
                model=settings.openai_model,
                latency_ms=int((time.monotonic() - t0) * 1000),
                error="Max iterations reached",
            )

        except Exception as e:
            logger.error(f"RAG chat failed: {e}")
            error_msg = f"æŠ±æ­‰ï¼Œå‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
            yield error_msg

            await self.audit.log(
                user_id=user_id,
                action="rag_chat",
                input_summary=messages[-1].get("content", "")[:200] if messages else "",
                model=settings.openai_model,
                latency_ms=int((time.monotonic() - t0) * 1000),
                error=str(e),
            )
